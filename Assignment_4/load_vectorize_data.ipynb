{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization parameters\n",
    "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
    "NGRAM_RANGE = (1, 2)\n",
    "\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "# Whether text should be split into word or character n-grams.\n",
    "# One of 'word', 'char'.\n",
    "TOKEN_MODE = 'word'\n",
    "\n",
    "# Minimum document/corpus frequency below which a token will be discarded.\n",
    "MIN_DOCUMENT_FREQUENCY = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_vectorize_train(df1 = pd.read_csv('./data/train_lyrics_1000.csv'), df2 = pd.read_csv('./data/valid_lyrics_200.csv')):\n",
    "    \n",
    "    df1 = df1[['lyrics','mood']]\n",
    "    df2 = df2[['lyrics','mood']]\n",
    "    df1['lyrics'] = df1['lyrics'].apply(lambda x : x.lower())\n",
    "    df2['lyrics'] = df2['lyrics'].apply(lambda y : y.lower())\n",
    "    df1.loc[:,'lyrics']= df1.loc[:,'lyrics'].apply(lambda x : re.sub('[^a-zA-z0-9\\s]','',x))\n",
    "    df2.loc[:,'lyrics']= df2.loc[:,'lyrics'].apply(lambda x : re.sub('[^a-zA-z0-9\\s]','',x))\n",
    "    df1['lyrics'] = df1['lyrics'].apply(lambda x : re.sub('[\\n]',' ',x))\n",
    "    df2['lyrics'] = df2['lyrics'].apply(lambda x : re.sub('[\\n]',' ',x))\n",
    "    df1['lyrics'] = df1['lyrics'].apply(lambda x : x.lstrip(' '))\n",
    "    df2['lyrics'] = df2['lyrics'].apply(lambda x : x.lstrip(' '))\n",
    "\n",
    "    #Removing StopWords\n",
    "    stop = stopwords.words('english')\n",
    "    df1['lyrics'] = df1['lyrics'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    df2['lyrics'] = df2['lyrics'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "    #Lemmatize\n",
    "    lem = WordNetLemmatizer()\n",
    "    df1['lyrics'] = df1['lyrics'].apply(lambda x :\" \".join([lem.lemmatize(word) for word in x.split()]))\n",
    "    df2['lyrics'] = df2['lyrics'].apply(lambda x :\" \".join([lem.lemmatize(word) for word in x.split()]))\n",
    "    df1['lyrics'] = df1['lyrics'].apply(lambda x :\" \".join([lem.lemmatize(word, pos='a') for word in x.split()]))\n",
    "    df1['lyrics'] = df1['lyrics'].apply(lambda x :\" \".join([lem.lemmatize(word, pos='n') for word in x.split()]))\n",
    "    df1['lyrics'] = df1['lyrics'].apply(lambda x :\" \".join([lem.lemmatize(word, pos='v') for word in x.split()]))\n",
    "    df2['lyrics'] = df2['lyrics'].apply(lambda x :\" \".join([lem.lemmatize(word, pos='a') for word in x.split()]))\n",
    "    df2['lyrics'] = df2['lyrics'].apply(lambda x :\" \".join([lem.lemmatize(word, pos='n') for word in x.split()]))\n",
    "    df2['lyrics'] = df2['lyrics'].apply(lambda x :\" \".join([lem.lemmatize(word, pos='v') for word in x.split()]))\n",
    "\n",
    "    # Label encoding\n",
    "    #le = LabelEncoder()\n",
    "    #Y_train = le.fit_transform(df1['mood'])\n",
    "    #Y_test = le.fit_transform(df2['mood'])\n",
    "    \n",
    "    \n",
    "    temp1 = []\n",
    "    for index,row in df1.iterrows():\n",
    "        temp.append(0 if df1['mood'][index] == 'sad' else 1)\n",
    "    Y_train = temp1\n",
    "    \n",
    "    temp2 = []\n",
    "    for index,row in df2.iterrows():\n",
    "        temp1.append(0 if df1['mood'][index] == 'sad' else 1)\n",
    "    Y_test = temp2\n",
    "    \n",
    "    \n",
    "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "    kwargs = {\n",
    "            'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
    "            'min_df': MIN_DOCUMENT_FREQUENCY,\n",
    "    }\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "    \n",
    "     # Learn vocabulary from training texts and vectorize training texts.\n",
    "    X_train = vectorizer.fit_transform(df1['lyrics'])\n",
    "\n",
    "    # Vectorize validation texts.\n",
    "    X_test = vectorizer.transform(df2['lyrics'])\n",
    "\n",
    "    # Select top 'k' of the vectorized features.\n",
    "    selector = SelectKBest(f_classif, k=min(TOP_K, X_train.shape[1]))\n",
    "    selector.fit(X_train, Y_train)\n",
    "    X_train = selector.transform(X_train).astype('float32').toarray()\n",
    "    X_test = selector.transform(X_test).astype('float32').toarray()\n",
    "    print(X_test) \n",
    "    return X_train, Y_train, X_test, Y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_vectorize_pred(df3):\n",
    "    df1 = pd.read_csv('./data/train_lyrics_1000.csv')\n",
    "    df1 = df1[['lyrics','mood']]\n",
    "    #df3 = df3['lyrics']\n",
    "    df1['lyrics'] = df1['lyrics'].apply(lambda x : x.lower())\n",
    "    df3['lyrics'] = df3['lyrics'].apply(lambda y : y.lower())\n",
    "    df1.loc[:,'lyrics']= df1.loc[:,'lyrics'].apply(lambda x : re.sub('[^a-zA-z0-9\\s]','',x))\n",
    "    df3.loc[:,'lyrics']= df3.loc[:,'lyrics'].apply(lambda x : re.sub('[^a-zA-z0-9\\s]','',x))\n",
    "    df1['lyrics'] = df1['lyrics'].apply(lambda x : re.sub('[\\n]',' ',x))\n",
    "    df3['lyrics'] = df3['lyrics'].apply(lambda x : re.sub('[\\n]',' ',x))\n",
    "    df1['lyrics'] = df1['lyrics'].apply(lambda x : x.lstrip(' '))\n",
    "    df3['lyrics'] = df3['lyrics'].apply(lambda x : x.lstrip(' '))\n",
    "\n",
    "    #Removing StopWords\n",
    "    stop = stopwords.words('english')\n",
    "    df1['lyrics'] = df1['lyrics'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    df3['lyrics'] = df3['lyrics'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "    #Lemmatize\n",
    "    lem = WordNetLemmatizer()\n",
    "    df1['lyrics'] = df1['lyrics'].apply(lambda x :\" \".join([lem.lemmatize(word) for word in x.split()]))\n",
    "    df3['lyrics'] = df3['lyrics'].apply(lambda x :\" \".join([lem.lemmatize(word) for word in x.split()]))\n",
    "    df1['lyrics'] = df1['lyrics'].apply(lambda x :\" \".join([lem.lemmatize(word, pos='a') for word in x.split()]))\n",
    "    df1['lyrics'] = df1['lyrics'].apply(lambda x :\" \".join([lem.lemmatize(word, pos='n') for word in x.split()]))\n",
    "    df1['lyrics'] = df1['lyrics'].apply(lambda x :\" \".join([lem.lemmatize(word, pos='v') for word in x.split()]))\n",
    "    df3['lyrics'] = df3['lyrics'].apply(lambda x :\" \".join([lem.lemmatize(word, pos='a') for word in x.split()]))\n",
    "    df3['lyrics'] = df3['lyrics'].apply(lambda x :\" \".join([lem.lemmatize(word, pos='n') for word in x.split()]))\n",
    "    df3['lyrics'] = df3['lyrics'].apply(lambda x :\" \".join([lem.lemmatize(word, pos='v') for word in x.split()]))\n",
    "\n",
    "    # Label encoding\n",
    "    #le = LabelEncoder()\n",
    "    #Y_train = le.fit_transform(df1['mood'])\n",
    "    \n",
    "    temp1 = []\n",
    "    for index,row in df1.iterrows():\n",
    "        temp.append(0 if df1['mood'][index] == 'sad' else 1)\n",
    "    Y_train = temp1\n",
    "\n",
    "    \n",
    "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "    kwargs = {\n",
    "            'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
    "            'min_df': MIN_DOCUMENT_FREQUENCY,\n",
    "    }\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "\n",
    "    # Learn vocabulary from training texts and vectorize training texts.\n",
    "    X_train = vectorizer.fit_transform(df1['lyrics'])\n",
    "\n",
    "    # Vectorize validation texts.\n",
    "    X_test = vectorizer.transform(df3['lyrics'])\n",
    "\n",
    "    # Select top 'k' of the vectorized features.\n",
    "    selector = SelectKBest(f_classif, k=min(TOP_K, X_train.shape[1]))\n",
    "    selector.fit(X_train, Y_train)\n",
    "    X_train = selector.transform(X_train).astype('float32').toarray()\n",
    "    X_pred = selector.transform(X_test).astype('float32').toarray()\n",
    "    \n",
    "    return X_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
